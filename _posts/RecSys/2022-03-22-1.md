---
    title : "[논문 리뷰] AutoRec: Autoencoders Meet Collaborative Filtering"
    excerpt: "AutoRec: Autoencoders Meet Collaborative Filtering 논문 리뷰 및 구현"
    toc : true
    toc_sticky : true
    toc_label : "목차"
    classes: wide
    use_math: true
    categories :
     - Paper review
    tag :
     - ReqSys
---

[논문 링크](https://users.cecs.anu.edu.au/~akmenon/papers/autorec/autorec-paper.pdf)

# Abstract

본 논문은 Collaborative Filter에 Autoencoder 방법론을 적용한 모델을 제안한다.

기존 Movielens와 Netfilx 데이터를 사용했을 때 MF(biased matrix factorization), RBM-CF, LLORMA 보다 가볍고 효율적인 학습 모델이다.

# 1. Introduction

AutoRec는 기존 CF 모델들에 비해 user와 item 간 embedding을 보다 잘표현하고 복잡도를 줄인 것이 특징이다.

논문 저자는 AutoRec이 기존의 CF 모델기반 모델들에 비해 표현력(representational)과 연산(computational) 측면에서 이점을 가지고 있다 주장한다.

# 2. The Autorec Model

점수(rating) 기반 협업 필터링(Collaborative filtering)에선 $m$ 명의 유저와 $n$ 개의 아이템의 rating $r^{u}, r^{i}$는 $R \in \mathbb{R}^{n}$ 에서 다음과 같이 표현된다.

- 각 user에 대해 $u \in U = \{1 ... m\}$ 다음과 같이 표현 $r^{u} = \{R_{u1} ... R_{un} \}$ 
- 각 item에 대해 $i \in I = \{1 ... n\}$ 다음과 같이 표현 $r^{i} = \{R_{1i} ... R_{mi} \}$

본 논문에서는 부분적으로 발견된  $r^{i}$,  $(r^{u})$ 에 대해 I-AutoRec **Item-based(user-based) autoencoder** 를 설계한다.  
$r^{i}$,  $(r^{u})$ 를 저차원(low-demensional)의 latent(hidden) 차원으로 사영시키고 비어있는 rating을 추론하기 위해 reconstruct(decode)한다.

학습 시에는 다음과 같은 RMES를 최소화 하는 loss를 사용한다.

$$\min_{\theta } \sum_{ r \in S}\left\| r - h(r;\theta)\right\|^{2}_{2}$$

- $S$ :  rating matrix ($\mathbb{R}^{d}$)에서 rating vector 집합
- $h(r ; \theta)$ : rating을 reconstruction 하는 함수 Autoencorder 함수
  
$h(r ; \theta)$ 는 다음과 같이 정의 된다  

$$
    h(r ; \theta) = f(W\cdot g(Vr + \mu) + b)
$$

- $f(\cdot), g(\cdot)$ : Activation function (Identiy, Sigmoid, ...)
- $\theta = \{W,V, \mu, b\}$ : 학습 파라미터

overfitting을 방지하기 위해 목적함수에 regularisation term 을 추가해줌  

$$
\min_{\theta} \sum_{ r \in S}\left\| r - h(r;\theta)\right\|^{2}_{O} + \frac{\lambda}{2} \cdot (\left\|W\right\|^{2}_{F} + \left\|V \right\|^{2}_{F})
$$

- $\left\|\cdot\right\|^{2}_{O}$ :  관측된 데이터에 대해서 만 RMES를 계산함을 의미

학습이 진행되는 파라미터의 수는 $2mk + m + k$ <- $(W + V + \mu + b)$

AutoRec를 통해 예측되는 rating 은 다음과 같다.

$$
\widehat{R}_{ui} = (h(r^(i); \hat{\theta}))_{u}
$$

![image](https://user-images.githubusercontent.com/10546369/159540080-9b65464b-e26a-4e78-8c05-97268b81cb61.png){: .align-center}

Item rating base AutoRec 모델은 위 그림과 같이 표현된다.


**AutoRec 모델이 다른 모델과 구분되는 점**

- RBM-based 모델과 비교했을 때
  
    - RBM-based 모델이  generative, probabilistic모델인 반면 AutoRec 는 discriminative model이다.
    - RBM-CF 는 log likelhood를 최소화 하지만 AutoRec는 RMES를 직접적으로 최소화 함
    - RBM은 Contrastive Divergence를 필요로 하지만, AutoRec는 비교적 빠른 역전파 방식(gradient-based backpropagation)을 사용
    - RBM은 이산적(discrete) rating에만 적용 가능하고 각 rating value에 대해 모든 학습 파라미터를 가짐. 반면 AutoRec는 적은 학습 파라미터를 사용(how?)  
        
        (원문) 
        AutoRec is agnostic to rand hence requires fewer parameters. Fewer parameters enables AutoRec to have less memory footprint and less prone to overfitting

# 3. Experimental evaluation

![image](https://user-images.githubusercontent.com/10546369/159548988-2a0786da-a3d2-4694-bd69-57896eba2fc2.png){: align-center}

- **I-AutoRec**(Item based AutoRec)가 기존의 CF 모델들 보다 적은 RMES 성능을 보임
- $f(\cdot), g(\cdot)$ 을 각각 Identiy, Sigmoid를 사용했을 때 더 좋은 성능을 보임

![image](https://user-images.githubusercontent.com/10546369/159540707-da3a576f-e775-40fc-9950-d8d1d4e7b7cf.png){: .align-center}

위 그래프를 통해 I-AutoRec의 hidden unit이 많아질수록 RMES 값이  안정적으로 줄어드는것을  확인할 수 있다.